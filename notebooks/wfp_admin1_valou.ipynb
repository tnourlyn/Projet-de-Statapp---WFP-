{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b8958c",
   "metadata": {},
   "source": [
    "# Donn√©es WFP - Admin 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c499444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syst√®me configur√©. Racine : c:\\Users\\valen\\OneDrive\\Documents\\GitHub\\Projet-de-Statapp---WFP-\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# D√©tection automatique de la racine du projet\n",
    "# On remonte d'un niveau depuis le dossier 'notebooks'\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Ajout au path pour pouvoir faire \"from src...\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# D√©finition des chemins absolus\n",
    "DATA_RAW = os.path.join(project_root, \"data\", \"raw\")\n",
    "DATA_PROCESSED = os.path.join(project_root, \"data\", \"processed\")\n",
    "GEO_PATH = os.path.join(project_root, \"geo\")\n",
    "\n",
    "print(f\"Syst√®me configur√©. Racine : {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d327158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION DES CHEMINS\n",
    "# ==============================================================================\n",
    "# Ton fichier m√©t√©o (g√©n√©r√© √† l'√©tape pr√©c√©dente)\n",
    "fldas_path = \"../data/raw/FLDAS_Nigeria_Admin2_Aggregated.csv\" \n",
    "# Le fichier WFP \n",
    "wfp_path = \"../data/raw/WFP_food_security_data_nigeria.csv\"\n",
    "# Fichier de sortie\n",
    "output_path = \"../data/processed/NIGERIA_WFP_ADMIN1.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6300267f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Chargement et agr√©gation des donn√©es climatiques...\n",
      "   -> Donn√©es climatiques agr√©g√©es au niveau √âtat : (2664, 28)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. TRAITEMENT CLIMAT (ADMIN 2 -> ADMIN 1)\n",
    "# ==============================================================================\n",
    "print(\"1. Chargement et agr√©gation des donn√©es climatiques...\")\n",
    "df_clim = pd.read_csv(fldas_path)\n",
    "df_clim['time'] = pd.to_datetime(df_clim['time'])\n",
    "\n",
    "#On regroupe par DATE et par √âTAT (NAME_1)\n",
    "# On fait la moyenne de toutes les LGA (NAME_2) qui sont dans cet √©tat\n",
    "df_clim_admin1 = df_clim.groupby(['time', 'NAME_1']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "\n",
    "# 2. CORRECTION DES NOMS \n",
    "df_clim_admin1['NAME_1'] = df_clim_admin1['NAME_1'].replace({\n",
    "    'FederalCapitalTerritory': 'Abuja',  # Le plus important\n",
    "    'AkwaIbom': 'Akwa Ibom',             # Manque l'espace\n",
    "    'CrossRiver': 'Cross River',         # Manque l'espace\n",
    "    'Nasarawa': 'Nassarawa',             # Orthographe avec 2 's' souvent utilis√©e dans WFP\n",
    "    'Nassarawa': 'Nassarawa'             # S√©curit√© si d√©j√† avec 2 s\n",
    "})\n",
    "\n",
    "print(f\"   -> Donn√©es climatiques agr√©g√©es au niveau √âtat : {df_clim_admin1.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16606401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Pr√©paration des donn√©es WFP...\n",
      "   -> Donn√©es WFP pr√™tes : (1010, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 3. TRAITEMENT WFP (CIBLE)\n",
    "# ==============================================================================\n",
    "print(\"2. Pr√©paration des donn√©es WFP...\")\n",
    "df_wfp = pd.read_csv(wfp_path)\n",
    "\n",
    "# Cr√©ation d'une vraie colonne Date (1er du mois)\n",
    "# On prend l'ann√©e du 'quarter' (ex: \"2023Q1\" -> 2023) et le mois\n",
    "df_wfp['year'] = df_wfp['quarter'].str[:4].astype(int)\n",
    "df_wfp['time'] = pd.to_datetime(df_wfp[['year', 'month']].assign(day=1))\n",
    "\n",
    "# On renomme pour avoir la m√™me cl√© de jointure\n",
    "df_wfp = df_wfp.rename(columns={'ADM1_NAME': 'NAME_1'})\n",
    "\n",
    "# On garde les colonnes utiles\n",
    "cols_wfp = ['time', 'NAME_1', 'inadequate']\n",
    "df_wfp_clean = df_wfp[cols_wfp]\n",
    "\n",
    "print(f\"   -> Donn√©es WFP pr√™tes : {df_wfp_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24ed0057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Fusion des deux fichiers...\n",
      "------------------------------\n",
      "‚úÖ SUCC√àS ! Fichier fusionn√© sauvegard√© : ../data/processed/NIGERIA_WFP_ADMIN1.csv\n",
      "Dimensions finales : (973, 29)\n",
      "P√©riode couverte : 2023-01-01 au 2025-12-01\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>NAME_1</th>\n",
       "      <th>inadequate</th>\n",
       "      <th>bnds</th>\n",
       "      <th>Y</th>\n",
       "      <th>X</th>\n",
       "      <th>Evap_tavg</th>\n",
       "      <th>LWdown_f_tavg</th>\n",
       "      <th>Lwnet_tavg</th>\n",
       "      <th>Psurf_f_tavg</th>\n",
       "      <th>...</th>\n",
       "      <th>Tair_f_tavg</th>\n",
       "      <th>Wind_f_tavg</th>\n",
       "      <th>SoilMoi00_10cm_tavg</th>\n",
       "      <th>SoilMoi10_40cm_tavg</th>\n",
       "      <th>SoilMoi40_100cm_tavg</th>\n",
       "      <th>SoilMoi100_200cm_tavg</th>\n",
       "      <th>SoilTemp00_10cm_tavg</th>\n",
       "      <th>SoilTemp10_40cm_tavg</th>\n",
       "      <th>SoilTemp40_100cm_tavg</th>\n",
       "      <th>SoilTemp100_200cm_tavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>Rivers</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.823889</td>\n",
       "      <td>6.987234</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>411.810330</td>\n",
       "      <td>-43.801556</td>\n",
       "      <td>100792.671190</td>\n",
       "      <td>...</td>\n",
       "      <td>299.888003</td>\n",
       "      <td>2.907054</td>\n",
       "      <td>0.405510</td>\n",
       "      <td>0.406853</td>\n",
       "      <td>0.417039</td>\n",
       "      <td>0.441677</td>\n",
       "      <td>299.747586</td>\n",
       "      <td>299.546677</td>\n",
       "      <td>299.288960</td>\n",
       "      <td>298.954850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>Sokoto</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13.043215</td>\n",
       "      <td>5.325762</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>335.779078</td>\n",
       "      <td>-115.339913</td>\n",
       "      <td>97790.403591</td>\n",
       "      <td>...</td>\n",
       "      <td>299.638528</td>\n",
       "      <td>6.335776</td>\n",
       "      <td>0.162425</td>\n",
       "      <td>0.324289</td>\n",
       "      <td>0.333883</td>\n",
       "      <td>0.367944</td>\n",
       "      <td>299.868678</td>\n",
       "      <td>299.929751</td>\n",
       "      <td>300.027598</td>\n",
       "      <td>300.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>Taraba</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.169318</td>\n",
       "      <td>10.847254</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>357.799072</td>\n",
       "      <td>-108.668333</td>\n",
       "      <td>96841.153500</td>\n",
       "      <td>...</td>\n",
       "      <td>301.657993</td>\n",
       "      <td>4.059479</td>\n",
       "      <td>0.196618</td>\n",
       "      <td>0.341409</td>\n",
       "      <td>0.351616</td>\n",
       "      <td>0.379171</td>\n",
       "      <td>302.027061</td>\n",
       "      <td>301.093865</td>\n",
       "      <td>300.262063</td>\n",
       "      <td>299.349812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>Yobe</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.280427</td>\n",
       "      <td>11.269654</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>326.325605</td>\n",
       "      <td>-121.448792</td>\n",
       "      <td>96968.869706</td>\n",
       "      <td>...</td>\n",
       "      <td>298.597382</td>\n",
       "      <td>6.557438</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>0.309747</td>\n",
       "      <td>0.319463</td>\n",
       "      <td>0.352666</td>\n",
       "      <td>299.414977</td>\n",
       "      <td>299.629153</td>\n",
       "      <td>299.892923</td>\n",
       "      <td>300.272008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>Zamfara</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.281723</td>\n",
       "      <td>6.331793</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>335.686780</td>\n",
       "      <td>-117.691993</td>\n",
       "      <td>96429.503643</td>\n",
       "      <td>...</td>\n",
       "      <td>299.681968</td>\n",
       "      <td>6.764253</td>\n",
       "      <td>0.150952</td>\n",
       "      <td>0.312316</td>\n",
       "      <td>0.324905</td>\n",
       "      <td>0.367590</td>\n",
       "      <td>300.202766</td>\n",
       "      <td>300.077284</td>\n",
       "      <td>299.990443</td>\n",
       "      <td>299.937183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time   NAME_1  inadequate  bnds          Y          X  Evap_tavg  \\\n",
       "968 2025-12-01   Rivers    0.160714   0.5   4.823889   6.987234   0.000034   \n",
       "969 2025-12-01   Sokoto    0.456522   0.5  13.043215   5.325762   0.000008   \n",
       "970 2025-12-01   Taraba    0.171429   0.5   8.169318  10.847254   0.000013   \n",
       "971 2025-12-01     Yobe    0.736111   0.5  12.280427  11.269654   0.000007   \n",
       "972 2025-12-01  Zamfara    0.326923   0.5  12.281723   6.331793   0.000011   \n",
       "\n",
       "     LWdown_f_tavg  Lwnet_tavg   Psurf_f_tavg  ...  Tair_f_tavg  Wind_f_tavg  \\\n",
       "968     411.810330  -43.801556  100792.671190  ...   299.888003     2.907054   \n",
       "969     335.779078 -115.339913   97790.403591  ...   299.638528     6.335776   \n",
       "970     357.799072 -108.668333   96841.153500  ...   301.657993     4.059479   \n",
       "971     326.325605 -121.448792   96968.869706  ...   298.597382     6.557438   \n",
       "972     335.686780 -117.691993   96429.503643  ...   299.681968     6.764253   \n",
       "\n",
       "     SoilMoi00_10cm_tavg  SoilMoi10_40cm_tavg  SoilMoi40_100cm_tavg  \\\n",
       "968             0.405510             0.406853              0.417039   \n",
       "969             0.162425             0.324289              0.333883   \n",
       "970             0.196618             0.341409              0.351616   \n",
       "971             0.148997             0.309747              0.319463   \n",
       "972             0.150952             0.312316              0.324905   \n",
       "\n",
       "     SoilMoi100_200cm_tavg  SoilTemp00_10cm_tavg  SoilTemp10_40cm_tavg  \\\n",
       "968               0.441677            299.747586            299.546677   \n",
       "969               0.367944            299.868678            299.929751   \n",
       "970               0.379171            302.027061            301.093865   \n",
       "971               0.352666            299.414977            299.629153   \n",
       "972               0.367590            300.202766            300.077284   \n",
       "\n",
       "     SoilTemp40_100cm_tavg  SoilTemp100_200cm_tavg  \n",
       "968             299.288960              298.954850  \n",
       "969             300.027598              300.237000  \n",
       "970             300.262063              299.349812  \n",
       "971             299.892923              300.272008  \n",
       "972             299.990443              299.937183  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# 4. FUSION (MERGE)\n",
    "# ==============================================================================\n",
    "print(\"3. Fusion des deux fichiers...\")\n",
    "\n",
    "# Inner join : on garde uniquement les lignes o√π on a LA M√âT√âO ET LA CIBLE\n",
    "df_final = pd.merge(df_wfp_clean, df_clim_admin1, on=['time', 'NAME_1'], how='inner')\n",
    "\n",
    "# Sauvegarde\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚úÖ SUCC√àS ! Fichier fusionn√© sauvegard√© : {output_path}\")\n",
    "print(f\"Dimensions finales : {df_final.shape}\")\n",
    "print(f\"P√©riode couverte : {df_final['time'].min().date()} au {df_final['time'].max().date()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Aper√ßu\n",
    "display(df_final.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f9080",
   "metadata": {},
   "source": [
    "# Fusion avec NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8374f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Conversion de WFP (Rapide)...\n",
      "‚úÖ WFP converti en Parquet.\n",
      "2. Conversion de NDVI (Morceau par morceau pour √©conomiser la RAM)...\n",
      "   Assemblage final...\n",
      "   Sauvegarde en Parquet...\n",
      "‚úÖ NDVI converti ! Taille sur le disque : 0.38 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Fichiers\n",
    "csv_ndvi = '../data/raw/NDVI.csv'\n",
    "parquet_ndvi = '../data/raw/NDVI.parquet'\n",
    "csv_wfp = '../data/processed/NIGERIA_WFP_ADMIN1.csv'\n",
    "parquet_wfp = '../data/processed/NIGERIA_WFP_ADMIN1.parquet'\n",
    "\n",
    "print(\"1. Conversion de WFP (Rapide)...\")\n",
    "# Le fichier WFP est petit, on le convertit direct\n",
    "df_wfp = pd.read_csv(csv_wfp)\n",
    "df_wfp['time'] = pd.to_datetime(df_wfp['time'])\n",
    "df_wfp.to_parquet(parquet_wfp, index=False)\n",
    "print(\"‚úÖ WFP converti en Parquet.\")\n",
    "\n",
    "print(\"2. Conversion de NDVI (Morceau par morceau pour √©conomiser la RAM)...\")\n",
    "# On lit le CSV par paquets de 100 000 lignes\n",
    "chunksize = 100000\n",
    "chunks = []\n",
    "\n",
    "# On d√©finit les types pour aider la lecture\n",
    "types_opti = {'adm_level': 'int8', 'vim': 'float32', 'viq': 'float32'}\n",
    "\n",
    "num = 0\n",
    "for chunk in pd.read_csv(csv_ndvi, sep=';', chunksize=chunksize, dtype=types_opti):\n",
    "    # On filtre tout de suite pour ne garder que ce qui est utile\n",
    "    chunk = chunk[chunk['adm_level'] == 1].copy()\n",
    "    \n",
    "    # Conversion date imm√©diate\n",
    "    chunk['date'] = pd.to_datetime(chunk['date'], dayfirst=True)\n",
    "    \n",
    "    # On ajoute √† la liste\n",
    "    chunks.append(chunk)\n",
    "    num += 1\n",
    "    if num % 10 == 0:\n",
    "        print(f\"   Traitement du bloc {num}...\")\n",
    "\n",
    "# On recolle les morceaux (concat)\n",
    "print(\"   Assemblage final...\")\n",
    "df_full = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Sauvegarde en Parquet\n",
    "print(\"   Sauvegarde en Parquet...\")\n",
    "df_full.to_parquet(parquet_ndvi, index=False, compression='snappy')\n",
    "print(f\"‚úÖ NDVI converti ! Taille sur le disque : {os.path.getsize(parquet_ndvi) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baffc0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement ultra-rapide des Parquets...\n",
      "NDVI charg√© : (31376, 4)\n",
      "Traitement...\n",
      "Fusion...\n",
      "‚úÖ Fini !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# On lit les fichiers Parquet\n",
    "print(\"Chargement ultra-rapide des Parquets...\")\n",
    "\n",
    "# L'avantage du Parquet : on ne charge QUE les colonnes utiles !\n",
    "# Plus besoin de charger 'adm_level' ou 'adm_id', on gagne une RAM √©norme.\n",
    "df_ndvi = pd.read_parquet(\n",
    "    '../data/raw/NDVI.parquet', \n",
    "    columns=['date', 'PCODE', 'vim', 'viq'] \n",
    ")\n",
    "\n",
    "df_wfp = pd.read_parquet('../data/processed/NIGERIA_WFP_ADMIN1.parquet')\n",
    "\n",
    "print(f\"NDVI charg√© : {df_ndvi.shape}\")\n",
    "\n",
    "# --- Traitement (Identique √† avant, mais bcp plus rapide) ---\n",
    "\n",
    "print(\"Traitement...\")\n",
    "# Cr√©ation mois (Dates d√©j√† au bon format gr√¢ce au Parquet !)\n",
    "df_ndvi['month'] = df_ndvi['date'] + pd.offsets.MonthBegin(-1)\n",
    "\n",
    "# Mapping\n",
    "mapping_etats = {\n",
    "    'NG001': 'Abia', 'NG002': 'Adamawa', 'NG003': 'Akwa Ibom', 'NG004': 'Anambra',\n",
    "    'NG005': 'Bauchi', 'NG006': 'Bayelsa', 'NG007': 'Benue', 'NG008': 'Borno',\n",
    "    'NG009': 'Cross River', 'NG010': 'Delta', 'NG011': 'Ebonyi', 'NG012': 'Edo',\n",
    "    'NG013': 'Ekiti', 'NG014': 'Enugu', 'NG015': 'Abuja',\n",
    "    'NG016': 'Gombe', 'NG017': 'Imo', 'NG018': 'Jigawa', 'NG019': 'Kaduna',\n",
    "    'NG020': 'Kano', 'NG021': 'Katsina', 'NG022': 'Kebbi', 'NG023': 'Kogi',\n",
    "    'NG024': 'Kwara', 'NG025': 'Lagos', 'NG026': 'Nasarawa', 'NG027': 'Niger',\n",
    "    'NG028': 'Ogun', 'NG029': 'Ondo', 'NG030': 'Osun', 'NG031': 'Oyo',\n",
    "    'NG032': 'Plateau', 'NG033': 'Rivers', 'NG034': 'Sokoto', 'NG035': 'Taraba',\n",
    "    'NG036': 'Yobe', 'NG037': 'Zamfara'\n",
    "}\n",
    "df_ndvi['state_name'] = df_ndvi['PCODE'].map(mapping_etats)\n",
    "\n",
    "# Agr√©gation\n",
    "ndvi_mensuel = df_ndvi.groupby(['month', 'state_name'])[['vim', 'viq']].mean().reset_index()\n",
    "\n",
    "# Fusion\n",
    "print(\"Fusion...\")\n",
    "df_final = pd.merge(\n",
    "    df_wfp,\n",
    "    ndvi_mensuel,\n",
    "    left_on=['time', 'NAME_1'],\n",
    "    right_on=['month', 'state_name'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_final.drop(columns=['month', 'state_name'], inplace=True, errors='ignore')\n",
    "\n",
    "# On remet le r√©sultat en CSV (ou en Parquet si tu pr√©f√®res garder la performance !)\n",
    "df_final.to_csv('NIGERIA_WFP_ADMIN1.csv', index=False)\n",
    "print(\"Fini !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a017df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DIAGNOSTIC TEMPOREL DES FICHIERS\n",
      "----------------------------------------\n",
      "üîπ WFP (Cible) : De 2023-01-01 √† 2025-12-01\n",
      "üîπ NDVI        : De 2002-07-01 √† 2026-01-11\n",
      "   ‚úÖ NDVI couvre toute la p√©riode !\n",
      "üîπ WATER       : De 2024-08-22 √† 2025-01-15\n",
      "   ‚ö†Ô∏è ATTENTION : WATER est incomplet par rapport au WFP.\n",
      "      -> Il manquera des donn√©es avant 2024-08-22\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "file_wfp = '../data/processed/NIGERIA_WFP_ADMIN1.csv'\n",
    "file_water = '../data/raw/water_area_NGA_adm2_daily.csv'\n",
    "file_ndvi = '../data/raw/NDVI.csv' # Ou .parquet si tu l'as converti\n",
    "\n",
    "print(\"üìä DIAGNOSTIC TEMPOREL DES FICHIERS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. Check WFP (Ton r√©f√©rentiel)\n",
    "df_wfp = pd.read_csv(file_wfp, usecols=['time'])\n",
    "df_wfp['time'] = pd.to_datetime(df_wfp['time'])\n",
    "start_wfp = df_wfp['time'].min()\n",
    "end_wfp = df_wfp['time'].max()\n",
    "print(f\"üîπ WFP (Cible) : De {start_wfp.date()} √† {end_wfp.date()}\")\n",
    "\n",
    "# 2. Check NDVI\n",
    "# On lit juste la colonne date pour aller vite\n",
    "try:\n",
    "    df_ndvi = pd.read_csv(file_ndvi, sep=';', usecols=['date'])\n",
    "    df_ndvi['date'] = pd.to_datetime(df_ndvi['date'], dayfirst=True)\n",
    "    print(f\"üîπ NDVI        : De {df_ndvi['date'].min().date()} √† {df_ndvi['date'].max().date()}\")\n",
    "    \n",
    "    # Verdict NDVI\n",
    "    if df_ndvi['date'].min() <= start_wfp and df_ndvi['date'].max() >= end_wfp:\n",
    "        print(\"   ‚úÖ NDVI couvre toute la p√©riode !\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è NDVI ne couvre pas tout (risque de NaNs sur les bords).\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur lecture NDVI: {e}\")\n",
    "\n",
    "# 3. Check Water\n",
    "try:\n",
    "    df_water = pd.read_csv(file_water, usecols=['date'])\n",
    "    df_water['date'] = pd.to_datetime(df_water['date'])\n",
    "    print(f\"üîπ WATER       : De {df_water['date'].min().date()} √† {df_water['date'].max().date()}\")\n",
    "    \n",
    "    # Verdict Water\n",
    "    if df_water['date'].min() <= start_wfp and df_water['date'].max() >= end_wfp:\n",
    "        print(\"   ‚úÖ WATER couvre toute la p√©riode !\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è ATTENTION : WATER est incomplet par rapport au WFP.\")\n",
    "        if df_water['date'].min() > start_wfp:\n",
    "            print(f\"      -> Il manquera des donn√©es avant {df_water['date'].min().date()}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur lecture WATER: {e}\")\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
